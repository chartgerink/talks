# Detecting data anomalies on the basis of summary statistics

Despite initiatives to increase data sharing, raw data underlying research articles are frequently unavailable (some even argue against it). Consequently, statistical methods to detect data anomalies in raw data are conditional on actually retrieving those raw data. However, all published research contains summary results that *are* available. In order to facilitate detection of data anomalies based on such summary statistics, we tested the performance of two statistical methods in 36 genuine- and 39 fabricated datasets on the anchoring effect ([osf.io/b24pq](https://osf.io/b24pq)). We inspected lack of variation in variances and anomalous amounts of high *p*-values. Considering we hardly know how researchers fabricate experimental data, we asked actual researchers to fabricate data instead of simulating datasets. We noticed that, as a group, fabricated nonsignificant effect sizes resembled genuine nonsignificant effects rather well. For fabricated significant effects, fabricators exaggerated effect sizes drastically. Upon analyzing individual responses, we refined the variance of variances method by altering the assumption from one underlying population variance to condition-specific variances. This greatly improved the performance of this method (AUC = .42 before, .77 after). Detecting data anomalies in nonsignificant results, based on excessive amounts of high *p*-values, performed barely better than chance (AUC = .52 and .53 for two different nonsignificant effects). These results indicate that researchers might be better at fabricating nonsignificant effects than fabricating significant effects (consciously or unconsciously so), that the variance of variances method proposed by Simonsohn (2013) is a fairly good method to detect data anomalies after minor adjustments to the assumptions, and that large effect sizes (r>.9) are a good first step to detecting data anomalies.